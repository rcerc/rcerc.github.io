{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d00678",
   "metadata": {},
   "source": [
    "# GRU and LSTM Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bdf1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: clang~=5.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: keras~=2.6 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.34.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./.venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.venv/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install or upgrade TensorFlow\n",
    "%pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789ab8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension to start and interface with TensorBoard\n",
    "# from Jupyter notebook\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf3e463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os import path # Filepath utilities\n",
    "from urllib import request # HTTP(S) requests\n",
    "import zlib # gzip (de)compression\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df2f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the gzipped tab-separated values dataset file\n",
    "URL_DS = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_00.tsv.gz'\n",
    "\n",
    "# Zero-based indices of elements in TSV file above\n",
    "INDEX_BODY = 13\n",
    "INDEX_RATING = 7\n",
    "\n",
    "# TensorBoard logs parent directory\n",
    "LOG_DIR_PARENT = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40759f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old TensorBoard logs (if any exist)\n",
    "%rm --recursive \"$LOG_DIR_PARENT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af510bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_lines():\n",
    "    ds = request.urlopen(URL_DS) # Send request\n",
    "    # gzip decompressor\n",
    "    decompressor = zlib.decompressobj(32 + zlib.MAX_WBITS)\n",
    "    line = [] # Will store decompressing line\n",
    "\n",
    "    for chunk in ds:\n",
    "        decompressed = decompressor.decompress(chunk)\n",
    "        if decompressed: # Only act if new data is available\n",
    "            split = decompressed.split(b'\\n') # Split on newlines\n",
    "            single = True # Will store whether `rv` is only one line\n",
    "            while True:\n",
    "                # Appends `line` with first element of `split`\n",
    "                line.append(split.pop(0))\n",
    "                # If no more lines exist, break\n",
    "                if len(split) < 1:\n",
    "                       break\n",
    "                yield b''.join(line) # Return line as a string\n",
    "                line = [] # Reset `line`\n",
    "\n",
    "\n",
    "def ds_tuples(line):\n",
    "    split = tf.strings.split(line, sep='\\t') # Split on tabs\n",
    "\n",
    "    body = split[INDEX_BODY]\n",
    "\n",
    "    rating = split[INDEX_RATING]\n",
    "    rating = tf.strings.to_number(rating)\n",
    "    # Decrement `rating` to match\n",
    "    # `tf.losses.SparseCategoricalCrossentropy`'s zero-based indexing\n",
    "    # expectation\n",
    "    rating -= 1\n",
    "\n",
    "    return body, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f79a6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds, size, is_train=True):\n",
    "    # Cache dataset in memory. You can pass a filename to `.cache`\n",
    "    # if you prefer caching on disk.\n",
    "    ds = ds.cache()\n",
    "    # The validation data does not have to be shuffled\n",
    "    if is_train:\n",
    "        # Reshuffle dataset every epoch\n",
    "        ds = ds.shuffle(size, reshuffle_each_iteration=True)\n",
    "    # Split dataset into batches of 100\n",
    "    ds = ds.batch(100)\n",
    "    # Prepare data (pass it through the input pipeline) before it is\n",
    "    # requested by the training model\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_datasets(size, split):\n",
    "    # Create new input pipeline from source\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        ds_lines, output_signature=tf.TensorSpec((), dtype=tf.string))\n",
    "    ds = ds.skip(1) # Skip the TSV headers\n",
    "    ds = ds.take(size)\n",
    "\n",
    "    # `split` is the fraction of the dataset that will be used for\n",
    "    # validation\n",
    "    val_size = int(size * split)\n",
    "\n",
    "    train_ds = ds.skip(val_size) # Skip `val_size` and take the rest\n",
    "    val_ds = ds.take(val_size) # Take `val_size` and ignore the rest\n",
    "\n",
    "    # Convert lines of TSV to input and target data.\n",
    "    # `num_parallel_calls=tf.data.AUTOTUNE` tells TensorFlow that it\n",
    "    # should can call `ds_tuples` more than ones at a time, depending\n",
    "    # on the CPU availability.\n",
    "    train_ds = train_ds.map(ds_tuples, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(ds_tuples, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = configure_for_performance(train_ds, size)\n",
    "    val_ds = configure_for_performance(val_ds, size, is_train=False)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "# Create datasets. Based on the arguments below, there will be\n",
    "# (1 - 0.2) * 10000 = 8000 training examples and 0.2 * 10000 = 2000\n",
    "# validation examples. If you are training your model on a low-end\n",
    "# computer, you should consider changing '10000' to '1000' or '500' so\n",
    "# training doesn't take too long.\n",
    "train_ds, val_ds = create_datasets(10000, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d12efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.TextVectorization()\n",
    "encoder.adapt(\n",
    "    train_ds.map(lambda t, r: t, num_parallel_calls=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb09db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential((\n",
    "    encoder,\n",
    "    # The `mask_zero` parameter below tells the embedder to interpret\n",
    "    # zeroes in the output of the `TextVectorization` layer as\n",
    "    # padding. In Keras, padding is used to indicate that certain\n",
    "    # timesteps are not applicable. This model will be trained with\n",
    "    # batches of varying data to descend the gradient of the loss\n",
    "    # function in a controlled manner. However, this demands that each\n",
    "    # text vector in the batch fed to the embedder has the same\n",
    "    # length. To do this, the `TextVectorization` layer adds zeroes to\n",
    "    # the end of each of the vectorised strings in the batch and we\n",
    "    # notify the embedder of this by passing `mask_zero=True` to it.\n",
    "    # The embedder then 'masks' these timesteps, as in, it ignores\n",
    "    # them and tells all following layers to do the same.\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=encoder.vocabulary_size(),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    # We are using a GRU because they are computationally more\n",
    "    # efficient and train better on smaller datasets. Feel free to\n",
    "    # replace the next code line with:\n",
    "\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "\n",
    "    # if you would rather use a generic LSTM network.\n",
    "\n",
    "    # The `reset_after=False` option used below tells Keras to create\n",
    "    # a GRU identical to the one we looked over in the post.\n",
    "    # By default, Keras uses a variation of the GRU (this can be\n",
    "    # disabled by passing `reset_after=False` to the `GRU`\n",
    "    # constructor) where, in the candidate gate, the element-wise\n",
    "    # product of the recurrent weights and the former cell state is\n",
    "    # multiplied by the reset gate activation vector instead of the\n",
    "    # former cell state being multiplied by the reset gate activation\n",
    "    # vector and then the recurrent weights. When the `use_bias`\n",
    "    # parameter is also true (which is the default), Keras also adds\n",
    "    # a bias to the product of the recurrent weights and the old cell\n",
    "    # state before multiplying it element-wise with the reset gate\n",
    "    # activation vector. These default options are two of the current\n",
    "    # requirements to use the cuDNN (optimised Nvidia GPU-accelerated\n",
    "    # backend) implementation of the GRU, so we will leave them be.\n",
    "\n",
    "    # We also pass the GRU layer through the Keras bidirectional layer\n",
    "    # to duplicate it and use both copies to construct a bidirectional\n",
    "    # (GRU) RNN.\n",
    "\n",
    "    # Also, the first parameter to the `GRU` class indicates the\n",
    "    # number of elements in the cell state vector and thus in the\n",
    "    # output of the GRU layer. Since we are using a bidirectional RNN,\n",
    "    # the number of output elements doubles, leading to 2 * 64 = 128\n",
    "    # outputs to the following dense layer.\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "    # The `Dense` layer class is a generic layer found in feedforward\n",
    "    # neural networks. The first parameter indicates the number of\n",
    "    # neurons that should exist in the layer.\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    # Treating Alice's problem as a classification problem instead of\n",
    "    # a regression problem (returning several values describing\n",
    "    # different qualities of the input) worked better, so we are using\n",
    "    # another dense layer to create a more or less one hot vector (a\n",
    "    # vector with the index of the highest value indicating the\n",
    "    # output) as the output of the network.\n",
    "    tf.keras.layers.Dense(5),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9b019c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=(tf.keras.metrics.SparseCategoricalAccuracy(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5dfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = path.join(LOG_DIR_PARENT, str(datetime.now()))\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a170f22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start TensorBoard and display interface inline\n",
    "%tensorboard --logdir \"$LOG_DIR_PARENT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68099a04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "80/80 [==============================] - 76s 921ms/step - loss: 1.0466 - sparse_categorical_accuracy: 0.7163 - val_loss: 0.8210 - val_sparse_categorical_accuracy: 0.7285\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 68s 858ms/step - loss: 0.6837 - sparse_categorical_accuracy: 0.7475 - val_loss: 0.7874 - val_sparse_categorical_accuracy: 0.7405\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 68s 827ms/step - loss: 0.5189 - sparse_categorical_accuracy: 0.7941 - val_loss: 0.8685 - val_sparse_categorical_accuracy: 0.7255\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 69s 859ms/step - loss: 0.3914 - sparse_categorical_accuracy: 0.8520 - val_loss: 0.9390 - val_sparse_categorical_accuracy: 0.7095\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 67s 849ms/step - loss: 0.2873 - sparse_categorical_accuracy: 0.8981 - val_loss: 1.1025 - val_sparse_categorical_accuracy: 0.7020\n",
      "Epoch 6/20\n",
      "80/80 [==============================] - 69s 860ms/step - loss: 0.2140 - sparse_categorical_accuracy: 0.9239 - val_loss: 1.1583 - val_sparse_categorical_accuracy: 0.7075\n",
      "Epoch 7/20\n",
      "80/80 [==============================] - 65s 820ms/step - loss: 0.1712 - sparse_categorical_accuracy: 0.9449 - val_loss: 1.2808 - val_sparse_categorical_accuracy: 0.6855\n",
      "Epoch 8/20\n",
      "80/80 [==============================] - 68s 850ms/step - loss: 0.1365 - sparse_categorical_accuracy: 0.9596 - val_loss: 1.3978 - val_sparse_categorical_accuracy: 0.7180\n",
      "Epoch 9/20\n",
      "80/80 [==============================] - 66s 832ms/step - loss: 0.1139 - sparse_categorical_accuracy: 0.9644 - val_loss: 1.4980 - val_sparse_categorical_accuracy: 0.6995\n",
      "Epoch 10/20\n",
      "80/80 [==============================] - 69s 862ms/step - loss: 0.0922 - sparse_categorical_accuracy: 0.9741 - val_loss: 1.7677 - val_sparse_categorical_accuracy: 0.6825\n",
      "Epoch 11/20\n",
      "80/80 [==============================] - 67s 844ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9764 - val_loss: 2.0116 - val_sparse_categorical_accuracy: 0.6735\n",
      "Epoch 12/20\n",
      "80/80 [==============================] - 67s 845ms/step - loss: 0.0738 - sparse_categorical_accuracy: 0.9786 - val_loss: 1.7745 - val_sparse_categorical_accuracy: 0.6940\n",
      "Epoch 13/20\n",
      "80/80 [==============================] - 69s 870ms/step - loss: 0.0667 - sparse_categorical_accuracy: 0.9809 - val_loss: 1.9231 - val_sparse_categorical_accuracy: 0.6855\n",
      "Epoch 14/20\n",
      "80/80 [==============================] - 66s 831ms/step - loss: 0.0626 - sparse_categorical_accuracy: 0.9812 - val_loss: 2.0656 - val_sparse_categorical_accuracy: 0.6925\n",
      "Epoch 15/20\n",
      "80/80 [==============================] - 68s 846ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9827 - val_loss: 2.0397 - val_sparse_categorical_accuracy: 0.7010\n",
      "Epoch 16/20\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.0556 - sparse_categorical_accuracy: 0.9824 - val_loss: 2.2568 - val_sparse_categorical_accuracy: 0.6895\n",
      "Epoch 17/20\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.0544 - sparse_categorical_accuracy: 0.9834 - val_loss: 2.1622 - val_sparse_categorical_accuracy: 0.7075\n",
      "Epoch 18/20\n",
      "80/80 [==============================] - 67s 836ms/step - loss: 0.0572 - sparse_categorical_accuracy: 0.9819 - val_loss: 2.2516 - val_sparse_categorical_accuracy: 0.6905\n",
      "Epoch 19/20\n",
      "80/80 [==============================] - 69s 863ms/step - loss: 0.0556 - sparse_categorical_accuracy: 0.9827 - val_loss: 2.0927 - val_sparse_categorical_accuracy: 0.6995\n",
      "Epoch 20/20\n",
      "80/80 [==============================] - 68s 852ms/step - loss: 0.0821 - sparse_categorical_accuracy: 0.9724 - val_loss: 1.8603 - val_sparse_categorical_accuracy: 0.7020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0db1e625e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=20, callbacks=(tensorboard_cb,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6702cc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:     I enjoyed reading this book because the characters were very unique. At one point or the other, I did find that the author went a little off-track, but it was an entertaining read for the most part.\n",
      "Prediction: 94.59% certain product was rated ****.\n",
      "Review:     This book definitely doesn't live up to the standards of the rest of the series. Not enough details and a very confusing storyline spoilt the whole thing.\n",
      "Prediction: 78.58% certain product was rated **.\n",
      "Review:     This explains how to create your own plant garden so well. The author was really thoughtful to include a couple seeds too! I gave it as a gift and the recipient started her own garden in less than a day.\n",
      "Prediction: 99.49% certain product was rated *****."
     ]
    }
   ],
   "source": [
    "def predict_rating(text):\n",
    "    # `model.predict` returns a list of predictions in the order the\n",
    "    # inputs were passed in. We only care about the first prediction\n",
    "    # because we only passed one in, so we index the first element\n",
    "    # with `[0]`.\n",
    "    one_hot = model.predict((text,))[0]\n",
    "    # Normalise `one_hot` such that sum of elements is 1\n",
    "    normalised = tf.nn.softmax(one_hot)\n",
    "    certainty = max(normalised)\n",
    "    percentage = certainty * 100\n",
    "    index = tf.argmax(one_hot) # Returns index of largest element\n",
    "    rating = index + 1 # Switch back to one-based indexing\n",
    "    # Use repeated asterisks for a neat representation of the rating\n",
    "    stars = rating * '*'\n",
    "    print(\n",
    "        f'Review:     {text}\\n'\n",
    "        f'Prediction: {percentage:.2f}% certain product was rated {stars}.'\n",
    "        )\n",
    "\n",
    "\n",
    "predict_rating('I enjoyed reading this book because the characters were very unique. At one point or the other, I did find that the author went a little off-track, but it was an entertaining read for the most part.')\n",
    "predict_rating(\"This book definitely doesn't live up to the standards of the rest of the series. Not enough details and a very confusing storyline spoilt the whole thing.\")\n",
    "predict_rating('This explains how to create your own plant garden so well. The author was really thoughtful to include a couple seeds too! I gave it as a gift and the recipient started her own garden in less than a day.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
